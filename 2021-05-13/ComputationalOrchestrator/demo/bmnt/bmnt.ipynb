{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/16/21 BMNT/KCA Demo\n",
    "# Based on JingWei's bmnt.ipynb notebook\n",
    "\n",
    "# (assumption that before we get here, we would have talked about Orchestrator access - \n",
    "# running in cloud for performance reasons, discussion about certificate warning, as well as \n",
    "# characterization of the Breast Cancer data set)\n",
    "\n",
    "# We will go through the Computation Orchestrator tool. Right now, as you can see, we use \n",
    "# Jupyter Lab as the interface to run Python scripts to get federated computation results from our\n",
    "# federated data set.\n",
    "\n",
    "# The current Orchestrator works with Python API - meaning any computation requires calling them using\n",
    "# our API. The product roadmap has a GUI version coming in the near future.\n",
    "\n",
    "# This part of the demo will be pretty \"technical\", in that you'll be executing actual Python code.\n",
    "# But don't worry, we've prepared the script for you so you don't have to come up with them or even type them,\n",
    "# and the tasks that we'll be performing are all pretty simple.\n",
    "\n",
    "# To execute the script in this \"notebook\", all you do is to make sure that the cursor is in the \"cell\" that\n",
    "# we want to execute, and hit \"shift-return\", and Jupyter Lab kernel will execute the script. \n",
    "\n",
    "# We're in the process of \"refactoring\" the API - meaning we're making it simpler to use, and also\n",
    "# making it work with a more powerful version of the Orchestrator and the Job Engine (that's the software that\n",
    "# actually runs all the SAFE Functions). We will be providing plenty of documentation and sample code so that\n",
    "# the KCA staff member who will be actually executing the queries will know how to make this all work. \n",
    "# And of course we will be there to help as well.\n",
    "\n",
    "# We have set up the demo to simulate the KCA use-case, but using 3 total enclaves: Hospital 1, Hospital 2, and KCA-Arm.\n",
    "# We split up the Breast Cancer data set into three - the data size is not equal (300, 200, 69), just to \n",
    "# simulate what we'll most likely see. \n",
    "\n",
    "# On with the Demo!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with basic setup - these are commands to the Python kernel to load in the necessary libraries\n",
    "# used for accessing the Orchestrator API. You don't have to worry about this too much, it's just boilerplate code,\n",
    "# and it should disappear as we polish the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sail.data import DataFrameGroup\n",
    "from sail.core import connect\n",
    "from sail.algo import FdLogistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So in this next step, we are going to have the Orchestrator connect to the Secure Computation Enclaves, \n",
    "# and then define the federated computation cluster. \n",
    "\n",
    "# (TODO: rename variables so it directly ties to hospital1, hospital2, kca)\n",
    "\n",
    "# By the way, you can see the connection credentials here, but we'll be moving this out of the script, and\n",
    "# making the user type in the username/password at connection time, for obvious security reasons. You can of course\n",
    "# use a password manager for that.\n",
    "\n",
    "# \"workplace\" is the temporary file location. That will disappear as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Secure Computation Enclave\n",
    "vm1 = connect(\"23.100.16.62\", 7000, \"lbart@igr.com\", \"sailpassword\")\n",
    "vm2 = connect(\"104.41.147.38\", 7000, \"lbart@igr.com\", \"sailpassword\")\n",
    "vm3 = connect(\"104.41.150.207\", 7000, \"lbart@igr.com\", \"sailpassword\")\n",
    "\n",
    "# Set up federated computation cluster \n",
    "vms = [vm1, vm2, vm3]\n",
    "\n",
    "# temporary file location\n",
    "workplace = \"/home/jjj/playground/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"vms\" contains the GUID (that's short for Globally Unique ID) - basically the identifiers for the \n",
    "# Secure Computation Enclaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['92443FA3CCE24C189D647717506BC2A4',\n",
       " 'CAA577ABAF294B98BEB6E6D16D79E985',\n",
       " 'FFC431D8360241EEADA87CD21C134DB3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will set up the actual federated data set. That is, the collection of tables from the three Enclaves.\n",
    "# Here, the \"[0,0,0]\" refers to the tables within the dataset. It's just taking the first table from the three\n",
    "# enclaves (because of zero-index), and putting them in the DataFrameGroup (which is the set of tables - i.e. the\n",
    "# Federated Data Set). \n",
    "\n",
    "# We're omitting in this demo, but normally you'd be able to ask the system about the data sets - e.g. name and description\n",
    "# of the tables and columns - and be able to reference them by name, not indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = DataFrameGroup(vms, workplace)\n",
    "dfg.import_data([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do some very simple \"exploration\" of the data. \n",
    "\n",
    "# First, we will look at \"class distribution\" across the hospitals. That is, the percentage of 'Benign' samples,\n",
    "# as a percentage of all samples. I just want to see how evenly the data sets are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5133333333333333, 0.755, 0.7536231884057971, 0.6274165202108963]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You'll see that there is a little bit of non-IIDness in terms of class distribution\n",
    "precentage = dfg.label_precentage('diagnosis', 'B', dfg.df)\n",
    "precentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were four numbers returned. The first three for the three data sets, and the last is the \n",
    "# \"federated percentage\", that is, the percentage of the entire federation. (it's a weighted average, by the way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll look at the distribution of the values of the data in each column. \n",
    "# We're just going to look at the per-hospital \"mean\" of each column just to see if there's anything unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.622979e+07</td>\n",
       "      <td>14.442953</td>\n",
       "      <td>19.319867</td>\n",
       "      <td>94.250900</td>\n",
       "      <td>683.729333</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>0.110838</td>\n",
       "      <td>0.099588</td>\n",
       "      <td>0.054394</td>\n",
       "      <td>0.184987</td>\n",
       "      <td>...</td>\n",
       "      <td>16.834210</td>\n",
       "      <td>25.902800</td>\n",
       "      <td>111.095200</td>\n",
       "      <td>940.108000</td>\n",
       "      <td>0.135359</td>\n",
       "      <td>0.275450</td>\n",
       "      <td>0.299886</td>\n",
       "      <td>0.124679</td>\n",
       "      <td>0.300126</td>\n",
       "      <td>0.085716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.223154e+07</td>\n",
       "      <td>13.896085</td>\n",
       "      <td>18.736000</td>\n",
       "      <td>90.140200</td>\n",
       "      <td>631.518000</td>\n",
       "      <td>0.092976</td>\n",
       "      <td>0.093612</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.042023</td>\n",
       "      <td>0.175944</td>\n",
       "      <td>...</td>\n",
       "      <td>15.803635</td>\n",
       "      <td>24.917050</td>\n",
       "      <td>104.003000</td>\n",
       "      <td>829.846000</td>\n",
       "      <td>0.126892</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0.241222</td>\n",
       "      <td>0.102931</td>\n",
       "      <td>0.280340</td>\n",
       "      <td>0.080871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.400473e+07</td>\n",
       "      <td>13.425014</td>\n",
       "      <td>20.763043</td>\n",
       "      <td>87.348841</td>\n",
       "      <td>597.239130</td>\n",
       "      <td>0.099128</td>\n",
       "      <td>0.107191</td>\n",
       "      <td>0.080527</td>\n",
       "      <td>0.045103</td>\n",
       "      <td>0.179654</td>\n",
       "      <td>...</td>\n",
       "      <td>15.162014</td>\n",
       "      <td>26.899855</td>\n",
       "      <td>100.035797</td>\n",
       "      <td>768.843478</td>\n",
       "      <td>0.135241</td>\n",
       "      <td>0.239342</td>\n",
       "      <td>0.241523</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.274594</td>\n",
       "      <td>0.085163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  radius_mean  texture_mean  perimeter_mean   area_mean  \\\n",
       "0  2.622979e+07    14.442953     19.319867       94.250900  683.729333   \n",
       "1  4.223154e+07    13.896085     18.736000       90.140200  631.518000   \n",
       "2  1.400473e+07    13.425014     20.763043       87.348841  597.239130   \n",
       "3  3.037183e+07    14.127292     19.289649       91.969033  654.889104   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0         0.097980          0.110838        0.099588             0.054394   \n",
       "1         0.092976          0.093612        0.075471             0.042023   \n",
       "2         0.099128          0.107191        0.080527             0.045103   \n",
       "3         0.096360          0.104341        0.088799             0.048919   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0       0.184987  ...     16.834210      25.902800       111.095200   \n",
       "1       0.175944  ...     15.803635      24.917050       104.003000   \n",
       "2       0.179654  ...     15.162014      26.899855       100.035797   \n",
       "3       0.181162  ...     16.269190      25.677223       107.261213   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0  940.108000          0.135359           0.275450         0.299886   \n",
       "1  829.846000          0.126892           0.227636         0.241222   \n",
       "2  768.843478          0.135241           0.239342         0.241523   \n",
       "3  880.583128          0.132369           0.254265         0.272188   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0              0.124679        0.300126                 0.085716  \n",
       "1              0.102931        0.280340                 0.080871  \n",
       "2              0.104651        0.274594                 0.085163  \n",
       "3              0.114606        0.290076                 0.083946  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_arr = dfg.df_mean(dfg.df)\n",
    "pd.DataFrame(mean_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, to do this \"correctly\", you can't just look at the mean and say everything is fine - \n",
    "# you'd want to look at the number of samples, look at variance, maybe even run some statistical tests\n",
    "# to see if they are IID, stuff like that. \n",
    "\n",
    "# We'll be expanding this \"exploration\" use-case to include more visual explorations as well - things like \n",
    "# histograms and correlation tables or heat maps and such. \n",
    "\n",
    "# Also, in this demo, we are omitting Cohort Selection (i.e. filter by gender or admission dates or any other criteria), \n",
    "# and also any data preprocessing (e.g. flattening of longitudinal data, imputing missing values, detecting\n",
    "# and removing outliers, or normalization/standardization that's often needed in certain statistical computations).\n",
    "# We'll be supporting those use-cases in the production system. \n",
    "\n",
    "# As this is a per-hospital aggregation result, as we discussed, we shouldn't be showing results if \n",
    "# the number of samples is very low. It's not a problem here, but when we do cohorts, we don't want to be\n",
    "# showing a mean value of a data set consisting of one subject!  We're working on what we are calling the \n",
    "# \"Privacy Sentinel\" - which will detect such potential privacy-leaking computation results, and either flag or \n",
    "# prevent such computation. Rules, including the threshold number of minimum sample size, will be configurable in \n",
    "# the system. Something that we'll discuss in the future in terms of details on how we want to set that up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any questions on data exploration phase?\n",
    "\n",
    "# So again, what we omitted today were:\n",
    "#   Visualization, cohort selection, longitudinal data processing, other data cleaning and pre-processing including\n",
    "#   normalization/standardization/mapping/transformation/etc, use of multiple tables, selection of columns to use\n",
    "\n",
    "# Ok, so now, on to statistical test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what we are going to do is to run a t-test, to see how the Malignant and Benign tumors are characterized\n",
    "# by the data set. \n",
    "# Now, I know this isn't the best test to run here, but we wanted to run a simple test against this limited data set.\n",
    "# So ignore the fact that this setup of the scientific inquiry is a classic case of p-value hacking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we're just going to set up the data set so that we split up the Benign and Malignant samples.\n",
    "# And then we'll run the Federated t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -0.94945105, -25.48762285, -10.88607835, -26.45930869,\n",
       "        -23.9911101 ,  -9.16195118, -17.73350088, -23.15059758,\n",
       "        -29.41628155,  -8.3534167 ,   0.30626599, -16.43275127,\n",
       "          0.19805554, -15.96984312, -15.6457782 ,   1.60214185,\n",
       "         -7.31032795,  -6.25614616, -10.6610848 ,   0.1555981 ,\n",
       "         -1.86534295, -29.40101867, -12.25246905, -30.02905861,\n",
       "        -25.77968773, -11.08669353, -17.48110286, -20.93651786,\n",
       "        -31.11302668, -10.9244796 ,  -8.16739497]),\n",
       " array([1.71397881e-001, 2.28554211e-096, 1.70986526e-025, 2.22091039e-101,\n",
       "        1.26685305e-088, 4.62784167e-019, 1.31779585e-056, 2.85786047e-084,\n",
       "        1.72387627e-116, 2.55825118e-016, 3.79757277e-001, 3.24291631e-050,\n",
       "        4.21536297e-001, 5.56740067e-048, 1.97675845e-046, 5.48405852e-002,\n",
       "        4.55845018e-013, 3.89020133e-010, 1.30067726e-024, 4.38202599e-001,\n",
       "        3.13248054e-002, 2.05962158e-116, 4.37180719e-031, 1.38333014e-119,\n",
       "        7.09000890e-098, 2.73752963e-026, 2.35289153e-055, 7.73643840e-073,\n",
       "        5.02375979e-125, 1.20609918e-025, 1.02815701e-015]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_df = dfg.df_select('diagnosis', 'B', dfg.df)\n",
    "b_df_no_diag = dfg.drop(['diagnosis','diagnosis','diagnosis'], [1,1,1], b_df)\n",
    "m_df = dfg.df_select('diagnosis', 'M', dfg.df)\n",
    "m_df_no_diag = dfg.drop(['diagnosis','diagnosis','diagnosis'], [1,1,1], m_df)\n",
    "tt_result = dfg.ttest_ind(b_df_no_diag, m_df_no_diag)\n",
    "tt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([3.76408575e-01, 1.62492217e-45, 2.14872862e-19, 8.31833771e-48,\n",
       "         6.46025624e-40, 5.84995816e-10, 2.63355755e-28, 3.35185652e-34,\n",
       "         5.80571651e-52, 4.78364914e-08, 4.85825145e-01, 1.61804039e-21,\n",
       "         2.86830402e-01, 4.38279479e-21, 1.48739378e-21, 5.80228217e-02,\n",
       "         5.15329114e-06, 2.51454934e-03, 7.11783890e-09, 1.94603540e-01,\n",
       "         2.85706389e-01, 3.22998774e-57, 3.07226678e-22, 7.05147308e-59,\n",
       "         4.85971357e-47, 2.23680556e-15, 9.69473973e-30, 1.12269872e-33,\n",
       "         8.69734087e-63, 1.20764436e-14, 3.09842477e-10]),\n",
       "  array([ -0.31521748, -16.85006577,  -9.5698641 , -17.4591834 ,\n",
       "         -15.35781645,  -6.28378501, -12.18996945, -13.81789851,\n",
       "         -18.5657272 ,  -5.46946799,  -0.03556844, -10.21991504,\n",
       "           0.5632903 , -10.08906542, -10.23093453,   1.57617847,\n",
       "          -4.48747688,  -2.82618055,  -5.83210326,   0.86231593,\n",
       "          -0.56659953, -19.9725185 , -10.43637941, -20.41999415,\n",
       "         -17.25534771,  -8.27077478, -12.59120233, -13.67494465,\n",
       "         -21.48006607,  -8.02130716,  -6.39493205])],\n",
       " [array([1.10428938e-01, 2.23606568e-42, 7.02127474e-09, 1.93630804e-44,\n",
       "         1.04180825e-40, 1.52715990e-10, 8.99754197e-25, 3.77221477e-41,\n",
       "         8.49761781e-52, 2.83287948e-06, 4.31454739e-01, 4.46962196e-23,\n",
       "         2.43452901e-01, 1.56335857e-21, 5.66306296e-19, 2.52039869e-01,\n",
       "         1.63805400e-06, 2.38016985e-09, 5.22978063e-16, 2.14974773e-01,\n",
       "         1.82352180e-02, 2.68226596e-47, 1.21917924e-09, 2.27580583e-48,\n",
       "         8.69451072e-40, 9.92310728e-13, 8.99748435e-19, 2.07473797e-30,\n",
       "         1.22700826e-47, 2.02803278e-07, 4.05932111e-05]),\n",
       "  array([ -1.22812516, -17.50579391,  -5.91935744, -18.19789662,\n",
       "         -16.95040065,  -6.63373912, -11.73093582, -17.09692187,\n",
       "         -20.72794989,  -4.6652711 ,   0.1728964 , -11.16811521,\n",
       "          -0.6965429 , -10.65060312,  -9.77760955,  -0.66930639,\n",
       "          -4.78914156,  -6.12649667,  -8.73189652,  -0.7908985 ,\n",
       "          -2.10593264, -19.16857142,  -6.2523916 , -19.536195  ,\n",
       "         -16.64514877,  -7.50954514,  -9.70801935, -13.57670096,\n",
       "         -19.28488243,  -5.24199652,  -4.0246712 ])],\n",
       " [array([1.33357760e-01, 1.61945689e-14, 7.86045171e-03, 3.59300250e-15,\n",
       "         4.96110892e-15, 8.14357236e-02, 1.23098663e-06, 8.88113935e-17,\n",
       "         8.41424605e-18, 3.91077291e-03, 1.54400482e-01, 1.64833025e-10,\n",
       "         4.14491835e-01, 6.47083463e-11, 1.52278659e-12, 1.58929492e-02,\n",
       "         1.33564284e-03, 3.27939523e-05, 5.50695534e-05, 1.46940057e-01,\n",
       "         3.27471537e-01, 4.31776697e-16, 1.97197905e-03, 8.80154339e-17,\n",
       "         7.41824579e-16, 2.01968312e-01, 1.04916554e-07, 3.48506004e-12,\n",
       "         4.93662057e-16, 3.90726767e-03, 5.95647077e-02]),\n",
       "  array([ -1.11999318,  -9.60176694,  -2.47844249,  -9.9718872 ,\n",
       "          -9.8923428 ,  -1.4109993 ,  -5.15167592, -10.89482873,\n",
       "         -11.49454175,  -2.74211507,   1.02553134,  -7.36910462,\n",
       "           0.21685068,  -7.59471071,  -8.49822184,   2.19296372,\n",
       "          -3.1194747 ,  -4.25873631,  -4.11001862,   1.05794476,\n",
       "          -0.448913  , -10.4978241 ,  -2.98604101, -10.89710076,\n",
       "         -10.36286788,  -0.83992691,  -5.78391368,  -8.29853291,\n",
       "         -10.46438221,  -2.74244331,  -1.57863696])]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_result_mono = dfg.ttest_ind_mono(b_df_no_diag, m_df_no_diag)\n",
    "tt_result_mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the result of the t-test is in two parts: the statistic and p-value.\n",
    "# The first array contains the statistic, and the second array contains the p-values.\n",
    "\n",
    "# Let's reformat that into a table with headers. We're using a lot of code to do the reformatting here, \n",
    "# but we'll be improving these federated function calls so that they return results that are easier for the user\n",
    "# to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D25896F6708B42D58694B25BD6D07957399B892847A743B3912272E4FA8CD060',\n",
       " '7BC1C7758FC446128B2A2DE7CFA4C489399B892847A743B3912272E4FA8CD060',\n",
       " '18E3E308502B421C9E8192A27A12875A399B892847A743B3912272E4FA8CD060']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = dfg.drop(['diagnosis','diagnosis','diagnosis'], [1,1,1], dfg.df)\n",
    "#h1_df_no_diag = df_new[0]\n",
    "#h2_df_no_diag = df_new[1]\n",
    "#kca_df_no_diag = df_new[2]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>statistic</th>\n",
       "      <td>-0.949451</td>\n",
       "      <td>-2.548762e+01</td>\n",
       "      <td>-1.088608e+01</td>\n",
       "      <td>-2.645931e+01</td>\n",
       "      <td>-2.399111e+01</td>\n",
       "      <td>-9.161951e+00</td>\n",
       "      <td>-1.773350e+01</td>\n",
       "      <td>-2.315060e+01</td>\n",
       "      <td>-2.941628e+01</td>\n",
       "      <td>-8.353417e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.940102e+01</td>\n",
       "      <td>-1.225247e+01</td>\n",
       "      <td>-3.002906e+01</td>\n",
       "      <td>-2.577969e+01</td>\n",
       "      <td>-1.108669e+01</td>\n",
       "      <td>-1.748110e+01</td>\n",
       "      <td>-2.093652e+01</td>\n",
       "      <td>-3.111303e+01</td>\n",
       "      <td>-1.092448e+01</td>\n",
       "      <td>-8.167395e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pvalue</th>\n",
       "      <td>0.171398</td>\n",
       "      <td>2.285542e-96</td>\n",
       "      <td>1.709865e-25</td>\n",
       "      <td>2.220910e-101</td>\n",
       "      <td>1.266853e-88</td>\n",
       "      <td>4.627842e-19</td>\n",
       "      <td>1.317796e-56</td>\n",
       "      <td>2.857860e-84</td>\n",
       "      <td>1.723876e-116</td>\n",
       "      <td>2.558251e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.059622e-116</td>\n",
       "      <td>4.371807e-31</td>\n",
       "      <td>1.383330e-119</td>\n",
       "      <td>7.090009e-98</td>\n",
       "      <td>2.737530e-26</td>\n",
       "      <td>2.352892e-55</td>\n",
       "      <td>7.736438e-73</td>\n",
       "      <td>5.023760e-125</td>\n",
       "      <td>1.206099e-25</td>\n",
       "      <td>1.028157e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   radius_mean  texture_mean  perimeter_mean     area_mean  \\\n",
       "statistic -0.949451 -2.548762e+01 -1.088608e+01   -2.645931e+01 -2.399111e+01   \n",
       "pvalue     0.171398  2.285542e-96  1.709865e-25   2.220910e-101  1.266853e-88   \n",
       "\n",
       "           smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "statistic    -9.161951e+00     -1.773350e+01   -2.315060e+01   \n",
       "pvalue        4.627842e-19      1.317796e-56    2.857860e-84   \n",
       "\n",
       "           concave points_mean  symmetry_mean  ...   radius_worst  \\\n",
       "statistic        -2.941628e+01  -8.353417e+00  ...  -2.940102e+01   \n",
       "pvalue           1.723876e-116   2.558251e-16  ...  2.059622e-116   \n",
       "\n",
       "           texture_worst  perimeter_worst    area_worst  smoothness_worst  \\\n",
       "statistic  -1.225247e+01    -3.002906e+01 -2.577969e+01     -1.108669e+01   \n",
       "pvalue      4.371807e-31    1.383330e-119  7.090009e-98      2.737530e-26   \n",
       "\n",
       "           compactness_worst  concavity_worst  concave points_worst  \\\n",
       "statistic      -1.748110e+01    -2.093652e+01         -3.111303e+01   \n",
       "pvalue          2.352892e-55     7.736438e-73         5.023760e-125   \n",
       "\n",
       "           symmetry_worst  fractal_dimension_worst  \n",
       "statistic   -1.092448e+01            -8.167395e+00  \n",
       "pvalue       1.206099e-25             1.028157e-15  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name = dfg.df_col_name(df_new)[0]\n",
    "tt_result_df = pd.DataFrame(columns = col_name)\n",
    "tt_result_df = tt_result_df.append(pd.DataFrame(tt_result[0].reshape(1,-1), columns = col_name), ignore_index = True)\n",
    "tt_result_df = tt_result_df.append(pd.DataFrame(tt_result[1].reshape(1,-1), columns = col_name), ignore_index = True)\n",
    "tt_result_df.rename(index={0:'statistic'},inplace=True)\n",
    "tt_result_df.rename(index={1:'pvalue'},inplace=True)\n",
    "tt_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here you see that the p-values of the test are tiny! 10^ -100?? \n",
    "# Is that really true? Did we messs something up? \n",
    "\n",
    "# You might realize at this point that we just used the Student's t-test, and one of the assumptions is that\n",
    "# the variance of the data points has to be similar across the two groups being tested. So let's just make sure \n",
    "# that assumption was met.\n",
    "\n",
    "# We'll run Federated Variance for all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.362815e+16</td>\n",
       "      <td>3.170222</td>\n",
       "      <td>15.961021</td>\n",
       "      <td>139.415582</td>\n",
       "      <td>18033.030100</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>...</td>\n",
       "      <td>3.925817</td>\n",
       "      <td>30.183536</td>\n",
       "      <td>182.982188</td>\n",
       "      <td>26765.425899</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.019703</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.901546e+16</td>\n",
       "      <td>10.265431</td>\n",
       "      <td>14.284393</td>\n",
       "      <td>477.625870</td>\n",
       "      <td>135378.355365</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>...</td>\n",
       "      <td>18.348967</td>\n",
       "      <td>29.537095</td>\n",
       "      <td>867.718099</td>\n",
       "      <td>357565.421850</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.029027</td>\n",
       "      <td>0.032945</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  radius_mean  texture_mean  perimeter_mean      area_mean  \\\n",
       "0  1.362815e+16     3.170222     15.961021      139.415582   18033.030100   \n",
       "1  1.901546e+16    10.265431     14.284393      477.625870  135378.355365   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0         0.000181          0.001139        0.001887             0.000253   \n",
       "1         0.000159          0.002915        0.005628             0.001182   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0       0.000615  ...      3.925817      30.183536       182.982188   \n",
       "1       0.000764  ...     18.348967      29.537095       867.718099   \n",
       "\n",
       "      area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0   26765.425899          0.000401           0.008497         0.019703   \n",
       "1  357565.421850          0.000478           0.029027         0.032945   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0              0.001281        0.001743                 0.000191  \n",
       "1              0.002144        0.005578                 0.000465  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance = []\n",
    "variance.append(dfg.df_var(b_df_no_diag))\n",
    "variance.append(dfg.df_var(m_df_no_diag))\n",
    "variance = pd.DataFrame(variance)\n",
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparable enough, but some look pretty different. Maybe we can understand more by taking the log difference in variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333115</td>\n",
       "      <td>1.17498</td>\n",
       "      <td>0.110982</td>\n",
       "      <td>1.231368</td>\n",
       "      <td>2.015868</td>\n",
       "      <td>0.128674</td>\n",
       "      <td>0.939547</td>\n",
       "      <td>1.092631</td>\n",
       "      <td>1.540854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.541999</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>1.556478</td>\n",
       "      <td>2.592207</td>\n",
       "      <td>0.177406</td>\n",
       "      <td>1.228482</td>\n",
       "      <td>0.514054</td>\n",
       "      <td>0.514871</td>\n",
       "      <td>1.163409</td>\n",
       "      <td>0.891096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        id  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      1  0.333115      1.17498      0.110982        1.231368   2.015868   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0         0.128674          0.939547        1.092631             1.540854   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...      1.541999        0.02165         1.556478    2.592207   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0          0.177406           1.228482         0.514054              0.514871   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0        1.163409                 0.891096  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so the variance for some of the columns are pretty different across the cohorts\n",
    "\n",
    "# to see the log difference of the variance among cohorts:\n",
    "# (base 2... so not quite a whole magnitude difference in variance,\n",
    "var_diff = np.abs(np.log(variance)[0:1].reset_index() - np.log(variance)[1:2].reset_index())\n",
    "var_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it comparable enough? So at this point, the researcher may decide to run a different flavor of t-test,\n",
    "# for example, Welch's un-equal variance t-test. But we'll just stop this path of inquiry here.\n",
    "\n",
    "# And we are now going to move on to something a little different - i.e. Federated Learning.\n",
    "# \"Learning\" in the sense that we will be actually training a model (you can say it's machine learning)\n",
    "# for classifying tumors. In this scenario, it would be to create a computational model, which takes in the \n",
    "# features obtained from single cell images from fine needle biopsy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with some setup. We'll be splitting up a training set, and test set.\n",
    "# The training set will be used to train the model, and then the test set will be used to \n",
    "# evaluate the model - we don't want to evaluate the model with the samples it has seen during training\n",
    "# because that's cheating; instead we will hold out some samples to be used exclusively for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = dfg.bin_cate(dfg.df, 'diagnosis')\n",
    "y_df = dfg.get_col(['diagnosis','diagnosis','diagnosis'], df_tmp)\n",
    "X_df = dfg.drop(['diagnosis','diagnosis','diagnosis'], [1,1,1], df_tmp)\n",
    "y_arr = dfg.to_numpy(y_df)\n",
    "X_arr = dfg.to_numpy(X_df)\n",
    "X_train = dfg.to_tensor(X_arr, 1)\n",
    "y_train = dfg.to_tensor(y_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['X_train']=[X_train[0], X_train[1]]\n",
    "data['y_train']=[y_train[0], y_train[1]]\n",
    "X_test = X_train[2]\n",
    "y_test = y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set up the Logistic Regression function.\n",
    "# There's a lot of code here - and again, we are improving the API to make it more user-friendly, so by the time\n",
    "# we get to production, we would be using far less code to run simple models like Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdl = FdLogistic([vm1, vm2], vm3, data, workplace)\n",
    "num_feature = dfg.df_get_feature_num(X_df[0])\n",
    "model = fdl.initmodel(num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y_predict = torch.sigmoid(self.linear(x))\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the the setup is complete, one thing to note is that we are using a framework called PyTorch\n",
    "# to run this Logistic Regression. It's a neural network framework used by Facebook to do very advanced\n",
    "# machine learning - they use this for all the face or image recoginition, for example. We'll also be using \n",
    "# established and proven statistical libraries to make sure that the computation results are reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now, let's Train the model. Since we are doing Federated Learning, it'll take longer than \n",
    "# it would for a \"pooled data\" model, in which one runs the algorithm on a single computer with all the data.\n",
    "\n",
    "# We'll set it up to do 100 iterations - that means the Secure Computation Enclaves will run their \"local\" computations,\n",
    "# and coordinate with the aggregator node up to 100 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fdl.train(model, num_feature, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now that it's done, let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9275362491607666"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = fdl.test(model, num_feature, X_test, y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So 94% accuracy! What that means is that we tested the model using the held out data - and used the model to predict\n",
    "# whether a sample was Benign or Malignant. And it got the prediction correct 91% of the time. \n",
    "\n",
    "# I'd say that's pretty good. Perhaps we can make the model perform even better by \"tuning the hyperparameters\" \n",
    "# things like \"learning rate\", \"maximum number of iterations\", \"convergence criteria\", etc.\n",
    "# But we'll omit all that - we'll keep this demo short for now. And we'll be showing a more sophisticated use-case \n",
    "# in what we're calling the \"KCA Stakeholder Demo\" to come at a later date, for the PIs and also Pharma, non-member \n",
    "# hospitals, and potentially even donors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
